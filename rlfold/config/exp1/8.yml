main:
    model: PPO2
    policy: CustomCnnPolicy
    n_workers: 30
    n_steps: 5000000
    multi_discrete: false
    frame_stack: 0
    seed: 1
models:
    PPO2:
        gamma: 1
        n_steps: 256
        ent_coef: 0.002
        learning_rate: 0.9e-04
        vf_coef: 0.5
        max_grad_norm: 0.5
        lam: 0.95
        nminibatches: 8
        noptepochs: 3
        cliprange: 0.1
        full_tensorboard_log: false
        verbose: 0
policies:
    CnnPolicy: {}
    CustomCnnPolicy: 
        filters:
        - 32
        - 256
        - 128
        - 16
        kernel_size:
        # - - 6
        - - 6
          - 1
        - - 1
          - 25
        - - 1
          - 10
        - - 1
          - 5
        # - - 1
        #   - 5
        stride:
        - 1
        - 1
        - 1
        - 1
        lstm: []
        shared:
        - 256
        - 128
        h_actor:
        - 64
        h_critic:
        - 16
        activ: relu
        pd_init_scale: 0.05
        conv_init_scale: 1.4
        kernel_initializer: glorot_normal_initializer
        init_bias: 0.5
    CustomCnnLstmPolicy: {}
    MlpPolicy: {}

environment:
    # Encoding
    encoding_type: 0
    use_nucleotides: true
    use_padding: true
    full_state: true
    kernel_size: 35

    # Reward
    detailed_comparison: true
    reward_exp: 9
    write_threshold: 0
    permute: false

    # Data
    meta_learning: true
    seq_count: 200
    randomize: true
    exact_seq_length: false
    seq_len:
        - 71
        - 99

testing:


