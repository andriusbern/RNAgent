main:
    model: PPO2
    policy: CustomCnnPolicy
    n_workers: 12
    n_steps: 2500000
    multi_discrete: false
    frame_stack: 0
    seed: 1
models:
    PPO2:
        gamma: 1
<<<<<<< HEAD
        n_steps: 256
        ent_coef: 0.01
=======
        n_steps: 64
        ent_coef: 0.002
>>>>>>> ea04851375f01ce9a0e5ea4f48ccdd69e7fde2b4
        learning_rate: 0.9e-04
        vf_coef: 0.5
        max_grad_norm: 0.5
        lam: 0.95
        nminibatches: 4
        noptepochs: 3
        cliprange: 0.1
        full_tensorboard_log: false
        verbose: 0
policies:
    CnnPolicy: {}
    CustomCnnPolicy: 
        filters:
<<<<<<< HEAD
        - 8
        - 128
        - 8
        kernel_size:
        -   - 6
            - 1
=======
        - 256
        - 256
        - 8
        kernel_size:
        -   - 6
            - 10
        -   - 1
            - 17
>>>>>>> ea04851375f01ce9a0e5ea4f48ccdd69e7fde2b4
        -   - 1
            - 25
        -   - 1
            - 15
        stride:
        - 1
        - 1
        - 1
        lstm: []
        shared:
        - 128
        h_actor:
        - 32
        h_critic:
        - 16
        activ: relu
        pd_init_scale: 0.05
        conv_init_scale: 1.4
        kernel_initializer: glorot_normal_initializer
        init_bias: 0.5
    CustomCnnLstmPolicy: {}
    MlpPolicy: {}
environment:
    kernel_size: 36
    reward_exp: 9
    full_state: true
    write_threshold: 0
    meta_learning: true
    seq_len:
<<<<<<< HEAD
        - 100
        - 125
        - 150
        - 175
        - 200
    seq_count: 200
=======
        - 82
        - 83
        - 84
        - 85
        - 86
        - 87
        - 88
        - 89
        - 90
    seq_count: 120
>>>>>>> ea04851375f01ce9a0e5ea4f48ccdd69e7fde2b4
    seq_nr: 0
    randomize: true

