main:
    model: PPO2
    policy: CustomCnnPolicy
    n_workers: 4
    n_steps: 1000000
    multi_discrete: false
    frame_stack: 0
models:
    PPO2:
        gamma: 0.995
        n_steps: 1024
        ent_coef: 0.01
        learning_rate: 0.0005
        vf_coef: 0.5
        max_grad_norm: 0.5
        lam: 0.95
        nminibatches: 32
        noptepochs: 3
        cliprange: 0.2
        full_tensorboard_log: true
        verbose: 0
    A2C: {}
    ACER: 
        gamma: 0.995
        n_steps: 20
        num_procs: 1
        q_coef: 0.5
        ent_coef: 0.05
        max_grad_norm: 10
        learning_rate: 0.0001
        lr_schedule: linear
        rprop_alpha: 0.99
        rprop_epsilon: 0.0001
        buffer_size: 5000
        replay_ratio: 4
        replay_start: 1000
        correction_term: 10.0
        trust_region: true
        alpha: 0.99
        delta: 1
        verbose: 0
    DDPG: 
        gamma: 0.99
        memory_policy: 
        eval_env:  
        nb_train_steps: 50
        nb_rollout_steps: 100
        nb_eval_steps: 100
        param_noise: 
        action_noise: 
        normalize_observations: true
        tau: 0.001
        batch_size: 128
        param_noise_adaption_interval: 50
        normalize_returns: false
        enable_popart: false
        observation_range: (-5.0, 5.0)
        critic_l2_reg: 0.0
        return_range: (-inf, inf)
        actor_lr: 0.0001
        critic_lr: 0.001
        clip_norm: None
        reward_scale: 1.0
        render: false
        render_eval: false
        memory_limit: 
        buffer_size: 50000
        random_exploration: 0.0
        verbose: 0
        tensorboard_log: 
        _init_setup_model: true
        full_tensorboard_log: false
policies:
    CnnPolicy: {}
    CustomCnnPolicy:
        filters:
        filters:
        - 16
        - 16
        - 16
        - 32
        - 32
        - 32
        kernel_size:
        - 3
        - 3
        - 3
        - 5
        - 5
        - 5
        stride:
        - 2
        - 1
        - 1
        - 2
        - 1
        - 1
        lstm: []
        shared: 
        - 512
        h_actor:
        - 128
        - 32
        h_critic:
        - 64
        - 16
        activ: relu
        pd_init_scale: 0.05
        conv_init_scale: 1.4
        kernel_initializer: glorot_normal_initializer
        init_bias: .5
    CustomCnnLstmPolicy: {}
    MlpPolicy: {}
    
drawer:
    # Queue parameters
    queue_bg_color: 10
    queue_image_size:
    - 12
    - 12
    queue_item_count: 3
    queue_type: column
    queue_orientation: 1
    queue_choice_indicator: true
    queue_scaling: 1.5
    queue_offset: 1.5
    random_queue: true
    
    # Height, layering
    base_intensity: 40
    height_scaling: 0.8
    layer_drawing: false
    layer_count: 2
    layer_threshold: 20
    
    # Items, load space, supports
    loadspace_bg_color: 10
    load_space_boundary: 0
    draw_supports: true
    highlight_group: false

    # Borders
    draw_borders: true
    border_color:
    - 0
    - 0
    - 0
    border_width: 1

    # Misc
    placement_correction: 7
    draw_weight: false
    draw_max_w_on_top: false
    random_pending_placement_position: true
    
environment:
    # Actions
    move_to_edge: false
    choose_item: true
    movement_step_size: 12
    
    # Env
    scale: 6.25
    step_limit: 150
    random_start: true
    load_space_scaling: 1
    max_height: 0
    verbose: true
    
    # Rewards
    area_fill_reward: 0
    edge_alignment_reward: 0.2
    base_reward: 0.5
    orientation_reward: 0
    centered_placement: 0
    end_of_episode: 0.2
    step_reward: -0.01
    interlocking_reward: 0.3
    change_item_reward: 0.005
    wall_alignment_reward: 0.2
    individual_stability: 0
    grouping_reward: 0.2