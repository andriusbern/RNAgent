main:
    model: ACER
    policy: CustomCnnPolicy
    n_workers: 1
    n_steps: 10000
    multi_discrete: False
models:
    PPO2:
        gamma: 0.9
        n_steps: 256
        ent_coef: 0.05
        learning_rate: 0.0005
        vf_coef: 0.5
        max_grad_norm: 0.5
        lam: 0.98
        nminibatches: 4
        noptepochs: 3
        cliprange: 0.2
        full_tensorboard_log: true
        verbose: 1
    A2C: {}
    ACER: 
        gamma: 0.99
        n_steps: 20
        num_procs: 1
        q_coef: 0.5
        ent_coef: 0.01
        max_grad_norm: 10
        learning_rate: 0.0007
        lr_schedule: linear
        rprop_alpha: 0.99
        rprop_epsilon: 0.0001
        buffer_size: 5000
        replay_ratio: 4
        replay_start: 1000
        correction_term: 10.0
        trust_region: true
        alpha: 0.99
        delta: 1
        verbose: 0
    DDPG: 
        gamma: 0.99
        memory_policy: None
        eval_env: None 
        nb_train_steps: 50
        nb_rollout_steps: 100
        nb_eval_steps: 100
        param_noise: None
        action_noise: None
        normalize_observations: True
        tau: 0.001
        batch_size: 128
        param_noise_adaption_interval: 50
        normalize_returns: False
        enable_popart: False
        observation_range: (-5.0, 5.0)
        critic_l2_reg: 0.0
        return_range: (-inf, inf)
        actor_lr: 0.0001
        critic_lr: 0.001
        clip_norm: None
        reward_scale: 1.0
        render: False
        render_eval: False
        memory_limit: None
        buffer_size: 50000
        random_exploration: 0.0
        verbose: 0
        tensorboard_log: None
        _init_setup_model: True
        full_tensorboard_log: False}
policies:
    CnnPolicy: {}
    CustomCnnPolicy:
        filters:
        filters:
        - 16
        - 16
        - 16
        - 32
        - 32
        - 32
        - 64
        - 64
        - 64
        kernel_size:
        - 3
        - 3
        - 3
        - 3
        - 3
        - 3
        - 3
        - 3
        - 3
        stride:
        - 2
        - 1
        - 1
        - 2
        - 1 
        - 1
        - 2
        - 1
        - 1
        lstm: []
        shared: []
        h_actor:
        
        h_critic:
        - 64
        activ: relu
        pd_init_scale: 0.01
        conv_init_scale: 1.412
        kernel_initializer: glorot_normal_initializer
        init_bias: 0.0
    CustomCnnLstmPolicy: {}
    MlpPolicy: {}
drawer:
    scale_factor: 5
    base_intensity: 50
    height_scaling: 1
    layer_drawing: false
    layer_count: 2
    layer_threshold: 20
    placement_correction: 8
    load_space_boundary: 0
    queue_item_count: 4
    queue_type: column
    queue_orientation: 1
    queue_choice_indicator: true
    queue_image_size:
    - 12
    - 12
    queue_scaling: 2
    queue_offset: 2
    draw_borders: true
    border_width: 1
    border_color:
    - 0
    - 0
    - 0
    loadspace_bg_color: 0
    queue_bg_color: 0
    draw_supports: true
    draw_weight: false
    draw_max_w_on_top: false
environment:
    random_start: false
    scale: 6.25
    load_space_scaling: 1
    use_image: true
    choose_item: true
    max_height: 0
    verbose: true
    area_fill_reward: 0
    edge_alignment_reward: 0
    base_reward: 1
    orientation_reward: 1
    centered_placement: 0
    end_of_episode: 0.2