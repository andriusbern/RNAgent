main:
  model: PPO2
  policy: CustomMlpPolicy
  n_workers: 1
  n_steps: 80000000
  seed: 1

models:
  PPO2:
    gamma: 1
    n_steps: 1024
    ent_coef: 0.01
    learning_rate: 0.0005
    vf_coef: 0.5
    max_grad_norm: 0.5
    lam: 0.98
    nminibatches: 16
    noptepochs: 4
    cliprange: 0.15
    full_tensorboard_log: false
    verbose: 0

environment:
  # Encoding
  verbose: false
  encoding_type: 2
  use_nucleotides: true
  use_padding: true
  full_state: true
  kernel_size: 25

  # Permutations
  permute: false
  permutation_budget: 50
  permutation_radius: 1
  permutation_threshold: 5
  allow_gu_permutations: false
  mutation_probability: 0.5

  # Reward
  detailed_comparison: true
  reward_exp: 9
  write_threshold: 0

  # Data
  meta_learning: true
  seq_count: 1
  randomize: true
  exact_seq_length: true
  test_set: false
  seq_len:
    - 55
    - 56

testing:
  evaluate_every: 2500000
  test_timeout: 60

policies:
  CnnPolicy: {}
  CnnLnLstmPolicy: {}
  CustomCnnPolicy:
    filters:
      - 512
      - 128
      - 16
    kernel_size:
      - - 10
        - 15
      - - 1
        - 10
      - - 1
        - 5
    stride:
      - 1
      - 1
      - 1
    lstm: []
    shared:
      - 256
      - 128
    h_actor:
      - 64
    h_critic:
      - 16
    activ: relu
    pd_init_scale: 0.05
    conv_init_scale: 1.4
    kernel_initializer: glorot_normal_initializer
    init_bias: 0.5
  CustomCnnLnLstmPolicy:
    filters:
      - 32
      - 128
      - 16
    kernel_size:
      - - 10
        - 1
      - - 1
        - 10
      - - 1
        - 5
    stride:
      - 1
      - 1
      - 1
    lstm:
    shared:
      - lstm
      - 128
    h_actor:
      - 64
    h_critic:
      - 16
    activ: relu
    pd_init_scale: 0.05
    conv_init_scale: 1.4
    kernel_initializer: glorot_normal_initializer
    init_bias: 0.5
  CustomCnnLstmPolicy: {}
  CustomMlpPolicy:
    shared:
      - 512
      - 512
    h_actor:
      - 64
      - 32
    h_critic:
      - 64
      - 32
